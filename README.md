# Scream Detection

This project implements a deep learning model to detect screams in audio, distinguishing them from ambient sounds and conversations using TensorFlow. The model achieves ~85% accuracy on a multi-class classification task (Screams, Ambient, Conversations) and includes interpretability analysis with SHAP.

üìÅ Project Structure
data/ ‚Äì All audio datasets

ambient/ ‚Äì Raw ambient audio (from UrbanSound8K)

ambient_converted/ ‚Äì Preprocessed ambient audio (16 kHz, mono)

screams/ ‚Äì Scream audio samples (74 WAV files)

conversations/ ‚Äì Conversation audio (~70 WAV files)

images/ ‚Äì Visual outputs for evaluation and interpretation

confusion_matrix.png ‚Äì Confusion matrix from evaluation

shap_class_0.png ‚Äì SHAP plot for Ambient class

shap_class_1.png ‚Äì SHAP plot for Screams class

shap_class_2.png ‚Äì SHAP plot for Conversations class

models/ ‚Äì Trained model files

scream_model.h5 ‚Äì Final trained model

src/ ‚Äì Source code and helper scripts

train.py ‚Äì Script to train the model

evaluate.py ‚Äì Model evaluation script

interpret.py ‚Äì SHAP interpretability tool

preprocess.py ‚Äì Audio preprocessing utilities

convert_audio.py ‚Äì Audio format conversion script

filter_urbansound8k.py ‚Äì Filter audio from UrbanSound8K

model.py ‚Äì CNN model architecture

UrbanSound8K/

FREESOUNDCREDITS.txt ‚Äì License and attribution info

metadata/UrbanSound8K.csv ‚Äì UrbanSound8K metadata

venv/ ‚Äì Python virtual environment (excluded from version control)

requirements.txt ‚Äì Python dependencies list

README.md ‚Äì Project overview and documentation


---

## üéß Dataset

- **Total**: 644 WAV files (mono, 16 kHz)
- **Classes**:
  - Screams: 74 samples
  - Ambient: ~500 samples (many from UrbanSound8K)
  - Conversations: 70 samples
- **Features**: MFCCs with shape `(128, 94)` reshaped to `(128, 94, 1)` for CNN input

> üì¶ **Source**: Ambient audio partially derived from the [UrbanSound8K dataset](https://urbansounddataset.weebly.com/urbansound8k.html). After downloading, place relevant files under `data/ambient/`. See `src/UrbanSound8K/FREESOUNDCREDITS.txt` for attributions.

---

## üìä Model Performance

- **Training Accuracy**: ~80.41% (with early stopping at 6 epochs)
- **Test Accuracy**: 85%
- **F1-Scores**:
  - Screams: 0.88
  - Ambient: 0.91
  - Conversations: 0.03 ‚ö†Ô∏è *(due to class imbalance)*

### Confusion Matrix
![Confusion Matrix](images/confusion_matrix.png)

### SHAP Interpretability
- **Ambient**:
  ![SHAP Ambient](images/shap_class_0.png)
- **Screams**:
  ![SHAP Screams](images/shap_class_1.png)
- **Conversations**:
  ![SHAP Conversations](images/shap_class_2.png)

---

## üß∞ Requirements

- Python 3.8+
- Dependencies (see `requirements.txt`):
  - `tensorflow`
  - `librosa`
  - `numpy`
  - `scikit-learn`
  - `matplotlib`
  - `shap`
  - `lime`
  - `pydub`
  - `ipython` *(optional, for SHAP visualizations)*

---

## ‚öôÔ∏è Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/devi-harikaa/scream-detector-project.git
   cd scream-detector-project
Create Virtual Environment

bash
Copy
Edit
python -m venv venv
source venv/bin/activate       # For Linux/Mac
.\venv\Scripts\activate        # For Windows
Install Dependencies

bash
Copy
Edit
pip install -r requirements.txt
Prepare Dataset

Download UrbanSound8K

Organize files into:

data/screams/

data/ambient_converted/

data/conversations/

Ensure all WAV files are 16 kHz, mono.

Use src/convert_audio.py and src/filter_urbansound8k.py as needed.

üß™ Usage
Train the Model
bash
Copy
Edit
python src/train.py
Model saved to models/scream_model.h5

Evaluate the Model
bash
Copy
Edit
python src/evaluate.py
Outputs evaluation metrics and saves images/confusion_matrix.png

Interpret the Model with SHAP
bash
Copy
Edit
python src/interpret.py
Generates SHAP plots for all three classes in images/
![shap_class_2](https://github.com/user-attachments/assets/25d5b8f8-096b-4f81-93e6-d00925b50fde)
![shap_class_1](https://github.com/user-attachments/assets/eb2d0ec9-785c-4aee-97d8-69e60c8ca1b1)
![shap_class_0](https://github.com/user-attachments/assets/367bf89d-7168-463a-852d-c1cd6331f129)
![confusion_matrix](https://github.com/user-attachments/assets/8a5cde2d-217d-451e-9bb7-02f4257a5c5b)

‚ö†Ô∏è Notes
Class Imbalance: Low F1 for Conversations is due to fewer training samples. Consider oversampling or using class weights in train.py.

SHAP Compatibility: Uses GradientExplainer to support TensorFlow models with batch normalization layers.

CPU/GPU: Runs on CPU by default. For GPU support, install CUDA 11.0, cuDNN 8.0, and tensorflow-gpu.

üîÆ Future Improvements
Augment dataset with more Screams and Conversations

Improve generalization with dropout/regularization

Use class weights for better balance during training

Extend project to support real-time scream detection from live microphone input

üìù License
MIT License

üì¨ Contact
For queries or suggestions, contact Neeharika at: harikadevi414@gmail.com
