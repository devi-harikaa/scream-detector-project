# Scream Detection

This project implements a deep learning model to detect screams in audio, distinguishing them from ambient sounds and conversations using TensorFlow. The model achieves ~85% accuracy on a multi-class classification task (Screams, Ambient, Conversations) and includes interpretability analysis with SHAP.
---
## üìÅ Project Structure
```bash
scream-detector-project/
‚îú‚îÄ‚îÄ data/                          # Audio data organized by class
‚îÇ   ‚îú‚îÄ‚îÄ ambient/                   # Raw ambient audio (from UrbanSound8K)
‚îÇ   ‚îú‚îÄ‚îÄ ambient_converted/        # Processed ambient audio (16 kHz, mono)
‚îÇ   ‚îú‚îÄ‚îÄ screams/                  # Scream audio samples (74 WAV files)
‚îÇ   ‚îú‚îÄ‚îÄ conversations/            # Conversation audio (~70 WAV files)
‚îÇ
‚îú‚îÄ‚îÄ images/                        # Visualizations and interpretation outputs
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png
‚îÇ   ‚îú‚îÄ‚îÄ shap_class_0.png          # SHAP plot - Ambient class
‚îÇ   ‚îú‚îÄ‚îÄ shap_class_1.png          # SHAP plot - Screams class
‚îÇ   ‚îú‚îÄ‚îÄ shap_class_2.png          # SHAP plot - Conversations class
‚îÇ
‚îú‚îÄ‚îÄ models/                        # Saved trained models
‚îÇ   ‚îî‚îÄ‚îÄ scream_model.h5           # Trained model file
‚îÇ
‚îú‚îÄ‚îÄ src/                           # Source code and utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ train.py                  # Model training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py               # Evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ interpret.py              # SHAP model interpretation
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py             # Data preprocessing utilities
‚îÇ   ‚îú‚îÄ‚îÄ convert_audio.py          # Audio conversion script
‚îÇ   ‚îú‚îÄ‚îÄ filter_urbansound8k.py    # Filter and select audio from UrbanSound8K
‚îÇ   ‚îú‚îÄ‚îÄ model.py                  # CNN model architecture
‚îÇ   ‚îî‚îÄ‚îÄ UrbanSound8K/
‚îÇ       ‚îú‚îÄ‚îÄ FREESOUNDCREDITS.txt  # UrbanSound8K license and credits
‚îÇ       ‚îî‚îÄ‚îÄ metadata/
‚îÇ           ‚îî‚îÄ‚îÄ UrbanSound8K.csv  # Dataset metadata
‚îÇ
‚îú‚îÄ‚îÄ venv/                          # Virtual environment (excluded from version control)
‚îÇ
‚îú‚îÄ‚îÄ README.md                      # Project documentation
‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies
```
---

## üéß Dataset

- **Total**: 644 WAV files (mono, 16 kHz)
- **Classes**:
  - Screams: 74 samples
  - Ambient: ~500 samples (many from UrbanSound8K)
  - Conversations: 70 samples
- **Features**: MFCCs with shape `(128, 94)` reshaped to `(128, 94, 1)` for CNN input

> üì¶ **Source**: Ambient audio partially derived from the [UrbanSound8K dataset](https://urbansounddataset.weebly.com/urbansound8k.html). After downloading, place relevant files under `data/ambient/`. See `src/UrbanSound8K/FREESOUNDCREDITS.txt` for attributions.

---

## üìä Model Performance

- **Training Accuracy**: ~80.41% (with early stopping at 6 epochs)
- **Test Accuracy**: 85%
- **F1-Scores**:
  - Screams: 0.88
  - Ambient: 0.91
  - Conversations: 0.03 ‚ö†Ô∏è *(due to class imbalance)*

### Confusion Matrix
![Confusion Matrix](images/confusion_matrix.png)

### SHAP Interpretability
- **Ambient**:
  ![SHAP Ambient](images/shap_class_0.png)
- **Screams**:
  ![SHAP Screams](images/shap_class_1.png)
- **Conversations**:
  ![SHAP Conversations](images/shap_class_2.png)

---

## üß∞ Requirements

- Python 3.8+
- Dependencies (see `requirements.txt`):
  - `tensorflow`
  - `librosa`
  - `numpy`
  - `scikit-learn`
  - `matplotlib`
  - `shap`
  - `lime`
  - `pydub`
  - `ipython` *(optional, for SHAP visualizations)*

---

## ‚öôÔ∏è Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/devi-harikaa/scream-detector-project.git
   cd scream-detector-project
Create Virtual Environment

```bash
Copy
Edit
python -m venv venv
source venv/bin/activate       # For Linux/Mac
.\venv\Scripts\activate        # For Windows
```
Install Dependencies

```bash
Copy
Edit
pip install -r requirements.txt
```
Prepare Dataset

Download UrbanSound8K

Organize files into:
```
data/screams/

data/ambient_converted/

data/conversations/

Ensure all WAV files are 16 kHz, mono.

Use src/convert_audio.py and src/filter_urbansound8k.py as needed.
```
üß™ Usage
Train the Model
```bash
Copy
Edit
python src/train.py
Model saved to models/scream_model.h5
```
Evaluate the Model
```bash
Copy
Edit
python src/evaluate.py
Outputs evaluation metrics and saves images/confusion_matrix.png
```


‚ö†Ô∏è Notes
Class Imbalance: Low F1 for Conversations is due to fewer training samples. Consider oversampling or using class weights in train.py.

SHAP Compatibility: Uses GradientExplainer to support TensorFlow models with batch normalization layers.

CPU/GPU: Runs on CPU by default. For GPU support, install CUDA 11.0, cuDNN 8.0, and tensorflow-gpu.

üîÆ Future Improvements
Augment dataset with more Screams and Conversations

Improve generalization with dropout/regularization

Use class weights for better balance during training

Extend project to support real-time scream detection from live microphone input

üìù License
MIT License

üì¨ Contact
For queries or suggestions, contact Neeharika at: harikadevi414@gmail.com
